{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6f24b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3776f544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix 1  \n",
      " [[4 3]\n",
      " [5 6]] \n",
      "\n",
      "Matrix 2 \n",
      " [[5 6]\n",
      " [7 8]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creation of 2 matrices \n",
    "mat_1 = np.array([[4,3], [5,6]])\n",
    "mat_2 = np.array([[5,6], [7,8]])\n",
    "print(\"Matrix 1  \\n\", mat_1, \"\\n\")\n",
    "print(\"Matrix 2 \\n\", mat_2, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55d941b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition: \n",
      " [[ 9  9]\n",
      " [12 14]] \n",
      "\n",
      "Subtraction: \n",
      " [[1 3]\n",
      " [2 2]]\n"
     ]
    }
   ],
   "source": [
    "# Addition and subtraction operation\n",
    "print(\"Addition: \\n\", np.add(mat_2, mat_1), \"\\n\")\n",
    "print(\"Subtraction: \\n\", np.subtract(mat_2, mat_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b7fd267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: \n",
      "(2, 2)\n",
      "\n",
      "Size: \n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# shape and size of array\n",
    "print(\"Shape: \")\n",
    "print(mat_1.shape)\n",
    "print(\"\")\n",
    "print(\"Size: \")\n",
    "print(mat_1.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f1d84f",
   "metadata": {},
   "source": [
    "### Understanding the difference between dense and sparse matrix \n",
    "The main difference between dense and sparse matrices lies in the way they store and represent data.\n",
    "\n",
    "A dense matrix is one where most of the elements are non-zero. In other words, it contains a significant number of non-zero values. Dense matrices are typically represented as 2D arrays, where each element of the array corresponds to a value in the matrix. These matrices are memory-intensive since they store all the elements, regardless of whether they are zero or non-zero. Operations on dense matrices are generally straightforward and efficient, as the data is contiguous in memory.\n",
    "\n",
    "On the other hand, a sparse matrix is one where the majority of elements are zero. In other words, it contains very few non-zero values. Sparse matrices are often encountered in real-world scenarios where data is inherently sparse, such as text data or social networks. To efficiently represent sparse matrices, various compression techniques are used. Instead of storing all the elements, sparse matrices store only the non-zero values along with their corresponding row and column indices. This representation reduces memory usage significantly, but it may lead to slower operations due to the need for additional computations to handle the compressed format.\n",
    "\n",
    "The choice between using a dense or sparse matrix representation depends on the specific characteristics of the data and the operations that need to be performed. Dense matrices are more suitable when most of the elements are non-zero and memory is not a major concern. Sparse matrices, on the other hand, are preferable when the data is sparse, as they offer memory efficiency and can provide computational advantages for certain algorithms tailored to exploit the sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4aa6f5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0], [0, 17], [16, 0]]\n",
      "  (1, 1)\t17\n",
      "  (2, 0)\t16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse\n",
    "dense_matrix = [[0,0], [0,17], [16,0]]\n",
    "print(dense_matrix)\n",
    "sparse_matrix = scipy.sparse.csr_matrix(dense_matrix)\n",
    "print(sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49011725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix: \n",
      "[[4 3]\n",
      " [5 6]]\n",
      "\n",
      "Transpose:\n",
      "[[4 5]\n",
      " [3 6]]\n"
     ]
    }
   ],
   "source": [
    "# transpose matrix \n",
    "print(\"Matrix: \")\n",
    "print(mat_1)\n",
    "print(\"\")\n",
    "print(\"Transpose:\")\n",
    "print(mat_1.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac74acd2",
   "metadata": {},
   "source": [
    "### What is Normalisation?\n",
    "Normalization, in the context of data processing and analysis, refers to the process of transforming data to a common scale or range. The goal of normalization is to bring the values of different variables or features onto a similar scale, enabling fair comparisons and eliminating biases that may arise due to varying magnitudes or units.\n",
    "\n",
    "Normalization is particularly important when dealing with datasets that contain features with significantly different scales. For example, consider a dataset that includes two features: \"age\" ranging from 0 to 100 and \"income\" ranging from 0 to 1,000,000. If these features are used in a machine learning model without normalization, the \"income\" feature may dominate the learning process due to its larger values, leading to biased results.\n",
    "\n",
    "There are various techniques for normalization, but two common methods are:\n",
    "\n",
    "**Min-Max Scaling (Normalization):** This technique rescales the data to a specified range, typically between 0 and 1. The formula for min-max scaling is:\n",
    "\n",
    "    X_normalized = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "Here, X represents the original value, X_min is the minimum value of the feature, and X_max is the maximum value of the feature. Min-max scaling preserves the relative relationships between the data points but compresses the range of values.\n",
    "\n",
    "**Z-Score Standardization:** This technique transforms the data to have a mean of 0 and a standard deviation of 1. It is also known as standardization. The formula for z-score standardization is:\n",
    "\n",
    "makefile\n",
    "\n",
    "    X_standardized = (X - X_mean) / X_std\n",
    "    \n",
    "Here, X represents the original value, X_mean is the mean of the feature, and X_std is the standard deviation of the feature. Z-score standardization centers the data around the mean and scales it by the standard deviation.\n",
    "\n",
    "The choice of normalization technique depends on the specific requirements of the data and the analysis or model being used. Normalization is commonly applied as a preprocessing step before feeding the data into machine learning algorithms to improve their performance and ensure fair comparisons between features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72cf4d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diagonal of matrix: \n",
      " [4 6]\n",
      "\n",
      "sum of diagnols:  10\n"
     ]
    }
   ],
   "source": [
    "# getting diagnol of the matrix\n",
    "print(\"diagonal of matrix: \\n\",mat_1.diagonal())\n",
    "print(\"\")\n",
    "# sum of the diagnols\n",
    "print(\"sum of diagnols: \", mat_1.diagonal().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0be6da7",
   "metadata": {},
   "source": [
    "### Imputing Techniques\n",
    "\n",
    "**Imputing techniques** in the context of data analysis and machine learning, refer to methods used to fill in missing or incomplete data values. Missing data can occur for various reasons, such as data collection errors, sensor failures, or incomplete survey responses. Imputing techniques aim to estimate or substitute missing values to maintain the integrity and usefulness of the data.\n",
    "\n",
    "**Mean/Median/Mode Imputation:** This technique involves replacing missing values with the mean (for numerical data), median (for data with outliers or skewed distributions), or mode (for categorical data) of the available values for that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23818c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  4.5\n",
      "Median:  4.5\n",
      "Standard Deviation:  1.118033988749895\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean: \", np.mean(mat_1))\n",
    "print(\"Median: \", np.median(mat_1))\n",
    "print(\"Standard Deviation: \", np.std(mat_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f7d1574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determinant:  9.000000000000002\n",
      "Matrix Multiplication: \n",
      " [[41 48]\n",
      " [67 78]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Determinant: \", np.linalg.det(mat_1))\n",
    "print(\"Matrix Multiplication: \\n\", np.matmul(mat_1, mat_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00821d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
